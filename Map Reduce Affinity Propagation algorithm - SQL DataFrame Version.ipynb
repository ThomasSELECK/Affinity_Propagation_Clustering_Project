{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from operator import add, sub\n",
    "\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc.stop()\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "#sc = SparkSession.builder \\\n",
    "#    .master(\"local\") \\\n",
    "#    .appName(\"Python Spark SQL\") \\\n",
    "#    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "SparkContext.setSystemProperty('spark.executor.cores', '4')\n",
    "\n",
    "sc = SparkContext(\"local\", \"App Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAFkCAYAAAC9wjgoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b24bf47f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temporary cell. To be deleted later\n",
    "\n",
    "N = 4\n",
    "\n",
    "x_1 = np.random.multivariate_normal(mean = [0,0], cov = [[0.1,0],[0,0.1]], size = N)\n",
    "x_2 = np.random.multivariate_normal(mean = [3,0], cov = [[0.3,-0.1],[-0.1,0.2]], size = N)\n",
    "x_3 = np.random.multivariate_normal(mean = [0,3], cov = [[0.2,0],[0,0.2]], size = N)\n",
    "N = 3 * N\n",
    "X = np.concatenate((x_1, x_2, x_3))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(X).to_csv(\"gaussian_data.txt\", sep=\"\\t\", header=None, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mat1 = sc.textFile(\"gaussian_data.txt\") \n",
    "#test = sqlContext.createDataFrame(mat1)\n",
    "from pyspark.sql.types import *\n",
    "lines = sc.textFile(\"gaussian_data.csv\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# Each line is converted to a tuple.\n",
    "data = parts.map(lambda p: (p[0], p[1], p[2]))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"x1 x2 x3\"\n",
    "\n",
    "fields = [StructField(field_name, FloatType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "spark_data = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of DataFrame[x1: float, x2: float, x3: float]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'flatMap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-73daa6d5b2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerateSimilarityMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-73daa6d5b2a0>\u001b[0m in \u001b[0;36mGenerateSimilarityMatrix\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[1;31m#mat1 = sc.textFile(\"gaussian_data.txt\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mnew_mat1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maddCoordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[1;31m# Generate a RDD for x coordinates and one for y coordinates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 841\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    842\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'flatMap'"
     ]
    }
   ],
   "source": [
    "# As we can't rely on data order in a Map/Reduce paradigm, we add the values' coordinates in each row.\n",
    "def addCoordinates(row):\n",
    "    \"\"\"\n",
    "    This function adds the matrix coordinates to each value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row: row of Spark RDD\n",
    "            This is one row of the current Spark RDD we're parsing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: list\n",
    "            This is the preprocessed row, with coordinates added.\n",
    "    \"\"\"\n",
    "    \n",
    "    values = row.split(\"\\t\")\n",
    "    index = int(values[0])\n",
    "    values = [float(_) for _ in values[1:]]\n",
    "    result = [[index, j, v] for j, v in enumerate(values)]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def computeSimilarity(row):\n",
    "    \"\"\"\n",
    "    This function adds the matrix coordinates to each value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row: row of Spark RDD\n",
    "            This is one row of the current Spark RDD we're parsing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    similarity: tuple\n",
    "            This is computed similarity, with matrix coordinates added.\n",
    "    \"\"\"\n",
    "    \n",
    "    ((i, v1), (j, v2)) = row\n",
    "    similarity = i, j, -1 * ((v1 - v2) ** 2)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def GenerateSimilarityMatrix():\n",
    "    \"\"\"\n",
    "    This method generates the similarity matrix for all the points given to the mapper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    S: Spark RDD\n",
    "            This is the computed similarity matrix in the Spark RDD format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the text file and add coordinates to each row\n",
    "    #mat1 = sc.textFile(\"gaussian_data.txt\")\n",
    "    mat1 = spark_data\n",
    "    new_mat1 = mat1.\n",
    "    #flatMap(addCoordinates)\n",
    "    \n",
    "    # Generate a RDD for x coordinates and one for y coordinates\n",
    "    new_mat_x = new_mat1.filter(lambda x: x[1] == 0).map(lambda row: (row[0], row[2]))\n",
    "    new_mat_y = new_mat1.filter(lambda x: x[1] == 1).map(lambda row: (row[0], row[2]))\n",
    "    \n",
    "    # Compute the cartesian product for x and y\n",
    "    mat_cartesian_x = new_mat_x.cartesian(new_mat_x)\n",
    "    mat_cartesian_y = new_mat_y.cartesian(new_mat_y)\n",
    "    \n",
    "    # Compute the similarity for x and y\n",
    "    sim_x = mat_cartesian_x.map(computeSimilarity)\n",
    "    sim_y = mat_cartesian_y.map(computeSimilarity)\n",
    "    \n",
    "    # Concatenate both x- and y-RDD and sum them by identical keys. Remove cells that equal zero\n",
    "    sim = sc.union([sim_x, sim_y])\n",
    "    final = sim.map(lambda row: ((row[0], row[1]), row[2])).reduceByKey(add).filter(lambda x: x[1] != 0)\n",
    "    \n",
    "    # Compute the matrix's diagonal\n",
    "    median = np.median(final.map(lambda row: row[1]).collect())\n",
    "    diagonal = sc.sparkContext.parallelize(range(mat1.count())).map(lambda x: ((x, x), median))\n",
    "    \n",
    "    # Generate the final matrix\n",
    "    S = sc.sparkContext.union([final, diagonal])\n",
    "    \n",
    "    return S\n",
    "\n",
    "S = GenerateSimilarityMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeR(A, R, S, N):\n",
    "    # Compute the sum of A and S\n",
    "    sumA_S = sc.sparkContext.union([A, S]).reduceByKey(add)\n",
    "\n",
    "    # Remove all rows where i == k and compute the maximum\n",
    "    tmp = sc.sparkContext.parallelize(range(N)).cartesian(sumA_S)\n",
    "    tmp = tmp.map(lambda row: (row[0], (row[1][0][1], (row[1][0][0], row[1][1])))).filter(lambda row: row[1][0] != row[0])\n",
    "    maximum = tmp.map(lambda row: ((row[1][1][0], row[0]), row[1][1][1])).reduceByKey(max)\n",
    "\n",
    "    # Compute r\n",
    "    r = sc.sparkContext.union([S, maximum]).reduceByKey(sub)\n",
    "\n",
    "    # Compute R\n",
    "    R = R.join(r).map(lambda row: (row[0], (1 - lambdaValue) * row[1][1] + lambdaValue * row[1][0]))\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeA(A, R, N):\n",
    "    # Remove all rows where i == k and compute the maximum\n",
    "    tmp = R.filter(lambda row: row[0][0] != row[0][1])\n",
    "    tmp = sc.sparkContext.parallelize(range(N)).cartesian(tmp).filter(lambda row: row[0] != row[1][0][0] and row[0] != row[1][0][1])\n",
    "    maximum = tmp.map(lambda row: (row[1][0], max(0, row[1][1]))).reduceByKey(add)\n",
    "\n",
    "    # Add R[k, k]\n",
    "    tmp = maximum.map(lambda row: ((row[0][1], row[0][1]), row)).join(R.filter(lambda row: row[0][0] == row[0][1]))\n",
    "    a = tmp.map(lambda row: (row[1][0][0], min(0, row[1][0][1] + row[1][1])))\n",
    "\n",
    "    # Compute the value of a when i == k\n",
    "    tmp = R.filter(lambda row: row[0][0] == row[0][1])\n",
    "    tmp = sc.sparkContext.parallelize(range(N)).cartesian(tmp).filter(lambda row: row[0] != row[1][0][1])\n",
    "    maximum = tmp.map(lambda row: (row[1][0], max(0, row[1][1]))).reduceByKey(add)\n",
    "\n",
    "    # Join both RDDs\n",
    "    a = sc.sparkContext.union([a, maximum])\n",
    "\n",
    "    # Compute A\n",
    "    A = A.join(a).map(lambda row: (row[0], (1 - lambdaValue) * row[1][1] + lambdaValue * row[1][0]))\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create A and R matrices\n",
    "A = sc.sparkContext.parallelize(range(N)).flatMap(lambda x: [((x, y), 0) for y in range(N)])\n",
    "R = sc.sparkContext.parallelize(range(N)).flatMap(lambda x: [((x, y), 0) for y in range(N)])\n",
    "lambdaValue = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iterations done\n",
      "2 iterations done\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o916.partitions.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.lang.Integer.valueOf(Unknown Source)\r\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:65)\r\n\tat scala.collection.IndexedSeqOptimized$class.zipWithIndex(IndexedSeqOptimized.scala:103)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.zipWithIndex(ArrayOps.scala:186)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:249)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:60)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-020d7c8d1dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iterations done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[1;31m# Update r(i, k) given a(i, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;31m# Update a(i, k) given r(i, k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fb2c52b5313e>\u001b[0m in \u001b[0;36mcomputeR\u001b[0;34m(A, R, S, N)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[1;31m# Compute r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaximum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[1;31m# Compute R\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m         \"\"\"\n\u001b[0;32m-> 1574\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1782\u001b[0m         \"\"\"\n\u001b[1;32m   1783\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2185\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o916.partitions.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\r\n\tat java.lang.Integer.valueOf(Unknown Source)\r\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:65)\r\n\tat scala.collection.IndexedSeqOptimized$class.zipWithIndex(IndexedSeqOptimized.scala:103)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.zipWithIndex(ArrayOps.scala:186)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:249)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:60)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.GeneratedMethodAccessor41.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for iter in range(iterations):\n",
    "    if iter % 2 == 0:\n",
    "        print(iter, \"iterations done\")\n",
    "    # Update r(i, k) given a(i, k)\n",
    "    R = computeR(A, R, S, N)\n",
    "    \n",
    "    # Update a(i, k) given r(i, k)\n",
    "    A = computeA(A, R, N)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
