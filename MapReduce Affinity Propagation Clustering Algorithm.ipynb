{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll implement the distributed version of the Affinity Propagation Clustering Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by testing Spark and checking everything is ok before proceeding... This example will be deleted later.\n",
    "An interesting link is the following: http://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin be implementing a word count example using Spark and mimicking Map/Reduce paradigm. We'll adapt this code to our problem later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SparkContext is automatically created in the object named \"sc\"\n",
    "\n",
    "# Read the text file\n",
    "text_file = sc.textFile(\"Les_Miserables.txt\")\n",
    "\n",
    "# Split the text into words\n",
    "tokenized = text_file.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Mapper part: Associate the weight 1 to each word\n",
    "wordCountsMapper = tokenized.map(lambda word: (word, 1))\n",
    "\n",
    "# Reducer part: Count the words occurences by grouping by key\n",
    "wordCountsReducer = wordCountsMapper.reduceByKey(lambda v1, v2: v1 + v2)\n",
    "\n",
    "# Display the counts\n",
    "print(wordCountsReducer.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual Affinity propagation Clustering algorithm :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class defines a mapper for the Affinity Propagation Clustering Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Mapper:\n",
    "    \"\"\"\n",
    "    This class defines a mapper object that computes the partial centroids for the partial dataset given in arguments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        This is the class' constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Spark RDD\n",
    "                This is the index i needed to select the first point.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Dataset\n",
    "        self.__data = data\n",
    "        \n",
    "        # Number of samples in the dataset\n",
    "        self.__N = data.count()\n",
    "        \n",
    "        # Availabilities matrix\n",
    "        self.__A = np.zeros((self.__N, self.__N))\n",
    "        \n",
    "        # Responsabilities matrix\n",
    "        self.__R = np.zeros((self.__N, self.__N))\n",
    "        \n",
    "        # Similarity matrix\n",
    "        self.__S = np.zeros((self.__N, self.__N))\n",
    "        \n",
    "        # List of centroids\n",
    "        self.__centers = []\n",
    "    \n",
    "    def a(self, i, k):\n",
    "        \"\"\"\n",
    "        This method computes the availability sent from point i to point k.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : integer\n",
    "                This is the index i needed to select the first point.\n",
    "\n",
    "        k : integer\n",
    "                This is the index k needed to select the first point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : float\n",
    "                This is the availability of point i for k.\n",
    "        \"\"\"\n",
    "\n",
    "        if i != k:\n",
    "            a = min([0, self.__R[k, k] + sum([max(0, self.__R[i_prime, k]) for i_prime in range(self.__N) if i_prime != i and i_prime != k])])\n",
    "        else:\n",
    "            a = sum([max(0, self.__R[i_prime, k]) for i_prime in range(self.__N) if i_prime != k])\n",
    "            \n",
    "        return a\n",
    "\n",
    "    def r(self, i, k):\n",
    "        \"\"\"\n",
    "        This method computes the responsability sent from point i to point k.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : integer\n",
    "                This is the index i needed to select the first point.\n",
    "\n",
    "        k : integer\n",
    "                This is the index k needed to select the first point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        r : float\n",
    "                This is the responsability of point i for k.\n",
    "        \"\"\"\n",
    "\n",
    "        r = self.__S[i, k] - max([self.__A[i, k_prime] + self.__S[i, k_prime] for k_prime in range(self.__N) if k_prime != k])\n",
    "        return r\n",
    "\n",
    "    def s(self, x_i, x_k):\n",
    "        \"\"\"\n",
    "        This method computes the similarity between two points (negative squared error).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_i : numpy array\n",
    "                This is the ith point of the dataset.\n",
    "\n",
    "        x_k : numpy array\n",
    "                This is the kth point of the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : float\n",
    "                This is the similarity between points i and k.\n",
    "        \"\"\"\n",
    "\n",
    "        s = -np.sum((x_i - x_k) ** 2)\n",
    "        return s\n",
    "\n",
    "    def __GenerateSimilarityMatrix(self):\n",
    "        \"\"\"\n",
    "        This method generates the similarity matrix for all the points given to the mapper.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Reformat data to make possible the use of cartesian method\n",
    "        tmp = sc.parallelize(self.__data.rdd.map(lambda x: np.array([e for e in x])).collect())\n",
    "        \n",
    "        # Compute the similarity matrix\n",
    "        similarityMatrix = tmp.zipWithIndex().cartesian(tmp.zipWithIndex()).map(lambda X: s(X[0][0], X[1][0]) if X[0][1] != X[1][1] else 0.0).collect()\n",
    "        self.__S = Matrices.dense(self.__N, self.__N, similarityMatrix).toArray()\n",
    "\n",
    "        # For diagonal: compute \"preferences\"\n",
    "        flatS = self.__S.flatten()\n",
    "        for i in range(self.__N):\n",
    "            S[i, i] = np.median(flatS[flatS != 0.0])\n",
    "                            \n",
    "    def ExecuteAffinityPropagation(self, iterations, lambdaValue = 0.5):\n",
    "        \"\"\"\n",
    "        This method executes the Affinity Propagation algorithm on several iterations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        iterations : positive integer\n",
    "                This is the number of iterations the algorithm will be executed.\n",
    "\n",
    "        lambdaValue : float\n",
    "                This is the lambda specified in the paper.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.__centers : list\n",
    "                This list contains all the centroids computed by the algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the similarity matrix\n",
    "        self.__GenerateSimilarityMatrix()\n",
    "\n",
    "        for it in range(iterations):\n",
    "            # Update r(i, k) given a(i, k)\n",
    "            for i in range(self.__N): # For each row\n",
    "                for k in range(self.__N): # For each column\n",
    "                    self.__R[i, k] = (1 - lambdaValue) * self.r(i, k) + lambdaValue * self.__R[i, k]\n",
    "\n",
    "            # Update a(i, k) given r(i, k)\n",
    "            for i in range(self.__N): # For each row\n",
    "                for k in range(self.__N): # For each column\n",
    "                    self.__A[i, k] = (1 - lambdaValue) * self.a(i, k) + lambdaValue * self.__A[i, k]\n",
    "\n",
    "            # Combine both a(i, k) and r(i, k) to get centers\n",
    "            self.__centers = [i for i in range(self.__N) if self.__R[i, i] + self.__A[i, i] > 0]\n",
    "            \n",
    "        return self.__centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the class into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a SparkSession # Only for Spark 2\n",
    "spark = SparkSession.builder.appName(\"Affinity Propagation algorithm\").getOrCreate()\n",
    "    \n",
    "# Read the data\n",
    "irisData = spark.read.option(\"header\",\"true\").csv(\"iris_species.csv\")\n",
    "\n",
    "# Only keep useful columns\n",
    "irisData = irisData.select(\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\")\n",
    "\n",
    "# Make sure we've the right data type\n",
    "irisData = irisData.withColumn(\"SepalLengthCm\", irisData[\"SepalLengthCm\"].cast(\"double\"))\n",
    "irisData = irisData.withColumn(\"SepalWidthCm\", irisData[\"SepalWidthCm\"].cast(\"double\"))\n",
    "irisData = irisData.withColumn(\"PetalLengthCm\", irisData[\"PetalLengthCm\"].cast(\"double\"))\n",
    "irisData = irisData.withColumn(\"PetalWidthCm\", irisData[\"PetalWidthCm\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how is our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many samples we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irisData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now split the dataframe to feed several mappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splittedDataframe = irisData.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2]) # We do 5 splits with 20% data in each\n",
    "#mappersArray = [Mapper(splittedDataframe[i]) for i in range(5)]\n",
    "\n",
    "# Maybe we can do a Spark version of this loop\n",
    "#for m in range(len(mappersArray)): print(mappersArray[m].ExecuteAffinityPropagation(100))\n",
    "from Mapper import *\n",
    "sc.addPyFile(\"Mapper.py\")\n",
    "mc = Mapper(splittedDataframe[0], sc)\n",
    "mc.ExecuteAffinityPropagation(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o70.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:272)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3dfde26cecf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msplittedDataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mirisData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We do 5 splits with 20% data in each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplittedDataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batchSize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbatched_serializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0mtempFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mreadRDDFromFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadRDDFromFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdump_stream\u001b[0;34m(self, iterator, stream)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdump_stream\u001b[0;34m(self, iterator, stream)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36m_write_with_length\u001b[0;34m(self, obj, stream)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_write_with_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mserialized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"serialized value should not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m'3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n\u001b[1;32m    322\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                     format(target_id, \".\", name, value))\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             raise Py4JError(\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o70.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:272)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
     ]
    }
   ],
   "source": [
    "from Mapper import *\n",
    "sc.addPyFile(\"Mapper.py\")\n",
    "\n",
    "splittedDataframe = irisData.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2]) # We do 5 splits with 20% data in each\n",
    "tmp = sc.parallelize([Mapper(splittedDataframe[i]) for i in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def s(x_i, x_k):\n",
    "    return -np.sum((x_i - x_k) ** 2)\n",
    "\n",
    "tmp = sc.parallelize(splittedDataframe[0].rdd.map(lambda x: np.array([e for e in x])).collect())\n",
    "res = tmp.zipWithIndex().cartesian(tmp.zipWithIndex()).map(lambda X: s(X[0][0], X[1][0]) if X[0][1] != X[1][1] else 0.0).collect()\n",
    "\n",
    "dim = splittedDataframe[0].count()\n",
    "S = Matrices.dense(dim, dim, res).toArray()\n",
    "\n",
    "# For diagonal: compute \"preferences\"\n",
    "flatS = S.flatten()\n",
    "for i in range(dim):\n",
    "    S[i, i] = np.median(flatS[flatS != 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1, 10))\n",
    "nrow = int(rdd.count() ** 0.5) # Compute number of rows\n",
    "\n",
    "res = rdd. \\\n",
    "zipWithIndex(). \\\n",
    "groupBy(lambda X: int(X[1] / nrow)). \\\n",
    "mapValues(lambda vals: [y[0] for y in sorted(vals, key=lambda y: y[0])]).map(lambda x: x[1]).collect()\n",
    "rdd.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For diagonal: compute \"preferences\"\n",
    "flatS = self.__S.flatten()\n",
    "for i in range(self.__N):\n",
    "    self.__S[i, i] = np.median(flatS[flatS != 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotData = splittedDataframe[0].select(\"SepalLengthCm\", \"SepalWidthCm\").toPandas()\n",
    "plt.scatter(plotData[\"SepalLengthCm\"], plotData[\"SepalWidthCm\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's map each data frame to a mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splittedDataframe[0].rdd.map(lambda x: (x, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temporary cell. To be deleted later\n",
    "\n",
    "N = 10\n",
    "x_1 = np.random.multivariate_normal(mean = [0,0], cov = [[0.1,0],[0,0.1]], size = N)\n",
    "x_2 = np.random.multivariate_normal(mean = [3,0], cov = [[0.3,-0.1],[-0.1,0.2]], size = N)\n",
    "x_3 = np.random.multivariate_normal(mean = [0,3], cov = [[0.2,0],[0,0.2]], size = N)\n",
    "N = 3 * N\n",
    "X = np.concatenate((x_1, x_2, x_3))\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()\n",
    "\n",
    "m1 = Mapper(X)\n",
    "m1Centroids = m1.ExecuteAffinityPropagation(100)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(X[:, 0], X[:, 1], s = 10, c = 'b', marker = \"s\", label = 'data')\n",
    "ax1.scatter(X[m1Centroids, 0], X[m1Centroids, 1], s = 30, c = 'r', marker = \"o\", label = 'centroids')\n",
    "plt.legend(loc = 'upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
